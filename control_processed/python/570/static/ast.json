{
  "type": "module",
  "start": {
    "row": 0,
    "column": 0
  },
  "end": {
    "row": 42,
    "column": 70
  },
  "text": "def setJobGroup(self, groupId, description, interruptOnCancel=False):\n        \"\"\"\n        Assigns a group ID to all the jobs started by this thread until the group ID is set to a\n        different value or cleared.\n\n        Often, a unit of execution in an application consists of multiple Spark actions or jobs.\n        Application programmers can use this method to group all those jobs together and give a\n        group description. Once set, the Spark web UI will associate such jobs with this group.\n\n        The application can use L{SparkContext.cancelJobGroup} to cancel all\n        running jobs in this group.\n\n        >>> import threading\n        >>> from time import sleep\n        >>> result = \"Not Set\"\n        >>> lock = threading.Lock()\n        >>> def map_func(x):\n        ...     sleep(100)\n        ...     raise Exception(\"Task should have been cancelled\")\n        >>> def start_job(x):\n        ...     global result\n        ...     try:\n        ...         sc.setJobGroup(\"job_to_cancel\", \"some description\")\n        ...         result = sc.parallelize(range(x)).map(map_func).collect()\n        ...     except Exception as e:\n        ...         result = \"Cancelled\"\n        ...     lock.release()\n        >>> def stop_job():\n        ...     sleep(5)\n        ...     sc.cancelJobGroup(\"job_to_cancel\")\n        >>> suppress = lock.acquire()\n        >>> suppress = threading.Thread(target=start_job, args=(10,)).start()\n        >>> suppress = threading.Thread(target=stop_job).start()\n        >>> suppress = lock.acquire()\n        >>> print(result)\n        Cancelled\n\n        If interruptOnCancel is set to true for the job group, then job cancellation will result\n        in Thread.interrupt() being called on the job's executor threads. This is useful to help\n        ensure that the tasks are actually stopped in a timely manner, but is off by default due\n        to HDFS-1208, where HDFS may respond to Thread.interrupt() by marking nodes as dead.\n        \"\"\"\n        self._jsc.setJobGroup(groupId, description, interruptOnCancel)",
  "children": [
    {
      "type": "function_definition",
      "start": {
        "row": 0,
        "column": 0
      },
      "end": {
        "row": 42,
        "column": 70
      },
      "text": "def setJobGroup(self, groupId, description, interruptOnCancel=False):\n        \"\"\"\n        Assigns a group ID to all the jobs started by this thread until the group ID is set to a\n        different value or cleared.\n\n        Often, a unit of execution in an application consists of multiple Spark actions or jobs.\n        Application programmers can use this method to group all those jobs together and give a\n        group description. Once set, the Spark web UI will associate such jobs with this group.\n\n        The application can use L{SparkContext.cancelJobGroup} to cancel all\n        running jobs in this group.\n\n        >>> import threading\n        >>> from time import sleep\n        >>> result = \"Not Set\"\n        >>> lock = threading.Lock()\n        >>> def map_func(x):\n        ...     sleep(100)\n        ...     raise Exception(\"Task should have been cancelled\")\n        >>> def start_job(x):\n        ...     global result\n        ...     try:\n        ...         sc.setJobGroup(\"job_to_cancel\", \"some description\")\n        ...         result = sc.parallelize(range(x)).map(map_func).collect()\n        ...     except Exception as e:\n        ...         result = \"Cancelled\"\n        ...     lock.release()\n        >>> def stop_job():\n        ...     sleep(5)\n        ...     sc.cancelJobGroup(\"job_to_cancel\")\n        >>> suppress = lock.acquire()\n        >>> suppress = threading.Thread(target=start_job, args=(10,)).start()\n        >>> suppress = threading.Thread(target=stop_job).start()\n        >>> suppress = lock.acquire()\n        >>> print(result)\n        Cancelled\n\n        If interruptOnCancel is set to true for the job group, then job cancellation will result\n        in Thread.interrupt() being called on the job's executor threads. This is useful to help\n        ensure that the tasks are actually stopped in a timely manner, but is off by default due\n        to HDFS-1208, where HDFS may respond to Thread.interrupt() by marking nodes as dead.\n        \"\"\"\n        self._jsc.setJobGroup(groupId, description, interruptOnCancel)",
      "children": [
        {
          "type": "def",
          "start": {
            "row": 0,
            "column": 0
          },
          "end": {
            "row": 0,
            "column": 3
          },
          "text": "def"
        },
        {
          "type": "identifier",
          "start": {
            "row": 0,
            "column": 4
          },
          "end": {
            "row": 0,
            "column": 15
          },
          "text": "setJobGroup"
        },
        {
          "type": "parameters",
          "start": {
            "row": 0,
            "column": 15
          },
          "end": {
            "row": 0,
            "column": 68
          },
          "text": "(self, groupId, description, interruptOnCancel=False)",
          "children": [
            {
              "type": "(",
              "start": {
                "row": 0,
                "column": 15
              },
              "end": {
                "row": 0,
                "column": 16
              },
              "text": "("
            },
            {
              "type": "identifier",
              "start": {
                "row": 0,
                "column": 16
              },
              "end": {
                "row": 0,
                "column": 20
              },
              "text": "self"
            },
            {
              "type": ",",
              "start": {
                "row": 0,
                "column": 20
              },
              "end": {
                "row": 0,
                "column": 21
              },
              "text": ","
            },
            {
              "type": "identifier",
              "start": {
                "row": 0,
                "column": 22
              },
              "end": {
                "row": 0,
                "column": 29
              },
              "text": "groupId"
            },
            {
              "type": ",",
              "start": {
                "row": 0,
                "column": 29
              },
              "end": {
                "row": 0,
                "column": 30
              },
              "text": ","
            },
            {
              "type": "identifier",
              "start": {
                "row": 0,
                "column": 31
              },
              "end": {
                "row": 0,
                "column": 42
              },
              "text": "description"
            },
            {
              "type": ",",
              "start": {
                "row": 0,
                "column": 42
              },
              "end": {
                "row": 0,
                "column": 43
              },
              "text": ","
            },
            {
              "type": "default_parameter",
              "start": {
                "row": 0,
                "column": 44
              },
              "end": {
                "row": 0,
                "column": 67
              },
              "text": "interruptOnCancel=False",
              "children": [
                {
                  "type": "identifier",
                  "start": {
                    "row": 0,
                    "column": 44
                  },
                  "end": {
                    "row": 0,
                    "column": 61
                  },
                  "text": "interruptOnCancel"
                },
                {
                  "type": "=",
                  "start": {
                    "row": 0,
                    "column": 61
                  },
                  "end": {
                    "row": 0,
                    "column": 62
                  },
                  "text": "="
                },
                {
                  "type": "false",
                  "start": {
                    "row": 0,
                    "column": 62
                  },
                  "end": {
                    "row": 0,
                    "column": 67
                  },
                  "text": "False"
                }
              ]
            },
            {
              "type": ")",
              "start": {
                "row": 0,
                "column": 67
              },
              "end": {
                "row": 0,
                "column": 68
              },
              "text": ")"
            }
          ]
        },
        {
          "type": ":",
          "start": {
            "row": 0,
            "column": 68
          },
          "end": {
            "row": 0,
            "column": 69
          },
          "text": ":"
        },
        {
          "type": "block",
          "start": {
            "row": 1,
            "column": 8
          },
          "end": {
            "row": 42,
            "column": 70
          },
          "text": "\"\"\"\n        Assigns a group ID to all the jobs started by this thread until the group ID is set to a\n        different value or cleared.\n\n        Often, a unit of execution in an application consists of multiple Spark actions or jobs.\n        Application programmers can use this method to group all those jobs together and give a\n        group description. Once set, the Spark web UI will associate such jobs with this group.\n\n        The application can use L{SparkContext.cancelJobGroup} to cancel all\n        running jobs in this group.\n\n        >>> import threading\n        >>> from time import sleep\n        >>> result = \"Not Set\"\n        >>> lock = threading.Lock()\n        >>> def map_func(x):\n        ...     sleep(100)\n        ...     raise Exception(\"Task should have been cancelled\")\n        >>> def start_job(x):\n        ...     global result\n        ...     try:\n        ...         sc.setJobGroup(\"job_to_cancel\", \"some description\")\n        ...         result = sc.parallelize(range(x)).map(map_func).collect()\n        ...     except Exception as e:\n        ...         result = \"Cancelled\"\n        ...     lock.release()\n        >>> def stop_job():\n        ...     sleep(5)\n        ...     sc.cancelJobGroup(\"job_to_cancel\")\n        >>> suppress = lock.acquire()\n        >>> suppress = threading.Thread(target=start_job, args=(10,)).start()\n        >>> suppress = threading.Thread(target=stop_job).start()\n        >>> suppress = lock.acquire()\n        >>> print(result)\n        Cancelled\n\n        If interruptOnCancel is set to true for the job group, then job cancellation will result\n        in Thread.interrupt() being called on the job's executor threads. This is useful to help\n        ensure that the tasks are actually stopped in a timely manner, but is off by default due\n        to HDFS-1208, where HDFS may respond to Thread.interrupt() by marking nodes as dead.\n        \"\"\"\n        self._jsc.setJobGroup(groupId, description, interruptOnCancel)",
          "children": [
            {
              "type": "expression_statement",
              "start": {
                "row": 1,
                "column": 8
              },
              "end": {
                "row": 41,
                "column": 11
              },
              "text": "\"\"\"\n        Assigns a group ID to all the jobs started by this thread until the group ID is set to a\n        different value or cleared.\n\n        Often, a unit of execution in an application consists of multiple Spark actions or jobs.\n        Application programmers can use this method to group all those jobs together and give a\n        group description. Once set, the Spark web UI will associate such jobs with this group.\n\n        The application can use L{SparkContext.cancelJobGroup} to cancel all\n        running jobs in this group.\n\n        >>> import threading\n        >>> from time import sleep\n        >>> result = \"Not Set\"\n        >>> lock = threading.Lock()\n        >>> def map_func(x):\n        ...     sleep(100)\n        ...     raise Exception(\"Task should have been cancelled\")\n        >>> def start_job(x):\n        ...     global result\n        ...     try:\n        ...         sc.setJobGroup(\"job_to_cancel\", \"some description\")\n        ...         result = sc.parallelize(range(x)).map(map_func).collect()\n        ...     except Exception as e:\n        ...         result = \"Cancelled\"\n        ...     lock.release()\n        >>> def stop_job():\n        ...     sleep(5)\n        ...     sc.cancelJobGroup(\"job_to_cancel\")\n        >>> suppress = lock.acquire()\n        >>> suppress = threading.Thread(target=start_job, args=(10,)).start()\n        >>> suppress = threading.Thread(target=stop_job).start()\n        >>> suppress = lock.acquire()\n        >>> print(result)\n        Cancelled\n\n        If interruptOnCancel is set to true for the job group, then job cancellation will result\n        in Thread.interrupt() being called on the job's executor threads. This is useful to help\n        ensure that the tasks are actually stopped in a timely manner, but is off by default due\n        to HDFS-1208, where HDFS may respond to Thread.interrupt() by marking nodes as dead.\n        \"\"\"",
              "children": [
                {
                  "type": "string",
                  "start": {
                    "row": 1,
                    "column": 8
                  },
                  "end": {
                    "row": 41,
                    "column": 11
                  },
                  "text": "\"\"\"\n        Assigns a group ID to all the jobs started by this thread until the group ID is set to a\n        different value or cleared.\n\n        Often, a unit of execution in an application consists of multiple Spark actions or jobs.\n        Application programmers can use this method to group all those jobs together and give a\n        group description. Once set, the Spark web UI will associate such jobs with this group.\n\n        The application can use L{SparkContext.cancelJobGroup} to cancel all\n        running jobs in this group.\n\n        >>> import threading\n        >>> from time import sleep\n        >>> result = \"Not Set\"\n        >>> lock = threading.Lock()\n        >>> def map_func(x):\n        ...     sleep(100)\n        ...     raise Exception(\"Task should have been cancelled\")\n        >>> def start_job(x):\n        ...     global result\n        ...     try:\n        ...         sc.setJobGroup(\"job_to_cancel\", \"some description\")\n        ...         result = sc.parallelize(range(x)).map(map_func).collect()\n        ...     except Exception as e:\n        ...         result = \"Cancelled\"\n        ...     lock.release()\n        >>> def stop_job():\n        ...     sleep(5)\n        ...     sc.cancelJobGroup(\"job_to_cancel\")\n        >>> suppress = lock.acquire()\n        >>> suppress = threading.Thread(target=start_job, args=(10,)).start()\n        >>> suppress = threading.Thread(target=stop_job).start()\n        >>> suppress = lock.acquire()\n        >>> print(result)\n        Cancelled\n\n        If interruptOnCancel is set to true for the job group, then job cancellation will result\n        in Thread.interrupt() being called on the job's executor threads. This is useful to help\n        ensure that the tasks are actually stopped in a timely manner, but is off by default due\n        to HDFS-1208, where HDFS may respond to Thread.interrupt() by marking nodes as dead.\n        \"\"\"",
                  "children": [
                    {
                      "type": "string_start",
                      "start": {
                        "row": 1,
                        "column": 8
                      },
                      "end": {
                        "row": 1,
                        "column": 11
                      },
                      "text": "\"\"\""
                    },
                    {
                      "type": "string_content",
                      "start": {
                        "row": 1,
                        "column": 11
                      },
                      "end": {
                        "row": 41,
                        "column": 8
                      },
                      "text": "\n        Assigns a group ID to all the jobs started by this thread until the group ID is set to a\n        different value or cleared.\n\n        Often, a unit of execution in an application consists of multiple Spark actions or jobs.\n        Application programmers can use this method to group all those jobs together and give a\n        group description. Once set, the Spark web UI will associate such jobs with this group.\n\n        The application can use L{SparkContext.cancelJobGroup} to cancel all\n        running jobs in this group.\n\n        >>> import threading\n        >>> from time import sleep\n        >>> result = \"Not Set\"\n        >>> lock = threading.Lock()\n        >>> def map_func(x):\n        ...     sleep(100)\n        ...     raise Exception(\"Task should have been cancelled\")\n        >>> def start_job(x):\n        ...     global result\n        ...     try:\n        ...         sc.setJobGroup(\"job_to_cancel\", \"some description\")\n        ...         result = sc.parallelize(range(x)).map(map_func).collect()\n        ...     except Exception as e:\n        ...         result = \"Cancelled\"\n        ...     lock.release()\n        >>> def stop_job():\n        ...     sleep(5)\n        ...     sc.cancelJobGroup(\"job_to_cancel\")\n        >>> suppress = lock.acquire()\n        >>> suppress = threading.Thread(target=start_job, args=(10,)).start()\n        >>> suppress = threading.Thread(target=stop_job).start()\n        >>> suppress = lock.acquire()\n        >>> print(result)\n        Cancelled\n\n        If interruptOnCancel is set to true for the job group, then job cancellation will result\n        in Thread.interrupt() being called on the job's executor threads. This is useful to help\n        ensure that the tasks are actually stopped in a timely manner, but is off by default due\n        to HDFS-1208, where HDFS may respond to Thread.interrupt() by marking nodes as dead.\n        "
                    },
                    {
                      "type": "string_end",
                      "start": {
                        "row": 41,
                        "column": 8
                      },
                      "end": {
                        "row": 41,
                        "column": 11
                      },
                      "text": "\"\"\""
                    }
                  ]
                }
              ]
            },
            {
              "type": "expression_statement",
              "start": {
                "row": 42,
                "column": 8
              },
              "end": {
                "row": 42,
                "column": 70
              },
              "text": "self._jsc.setJobGroup(groupId, description, interruptOnCancel)",
              "children": [
                {
                  "type": "call",
                  "start": {
                    "row": 42,
                    "column": 8
                  },
                  "end": {
                    "row": 42,
                    "column": 70
                  },
                  "text": "self._jsc.setJobGroup(groupId, description, interruptOnCancel)",
                  "children": [
                    {
                      "type": "attribute",
                      "start": {
                        "row": 42,
                        "column": 8
                      },
                      "end": {
                        "row": 42,
                        "column": 29
                      },
                      "text": "self._jsc.setJobGroup",
                      "children": [
                        {
                          "type": "attribute",
                          "start": {
                            "row": 42,
                            "column": 8
                          },
                          "end": {
                            "row": 42,
                            "column": 17
                          },
                          "text": "self._jsc",
                          "children": [
                            {
                              "type": "identifier",
                              "start": {
                                "row": 42,
                                "column": 8
                              },
                              "end": {
                                "row": 42,
                                "column": 12
                              },
                              "text": "self"
                            },
                            {
                              "type": ".",
                              "start": {
                                "row": 42,
                                "column": 12
                              },
                              "end": {
                                "row": 42,
                                "column": 13
                              },
                              "text": "."
                            },
                            {
                              "type": "identifier",
                              "start": {
                                "row": 42,
                                "column": 13
                              },
                              "end": {
                                "row": 42,
                                "column": 17
                              },
                              "text": "_jsc"
                            }
                          ]
                        },
                        {
                          "type": ".",
                          "start": {
                            "row": 42,
                            "column": 17
                          },
                          "end": {
                            "row": 42,
                            "column": 18
                          },
                          "text": "."
                        },
                        {
                          "type": "identifier",
                          "start": {
                            "row": 42,
                            "column": 18
                          },
                          "end": {
                            "row": 42,
                            "column": 29
                          },
                          "text": "setJobGroup"
                        }
                      ]
                    },
                    {
                      "type": "argument_list",
                      "start": {
                        "row": 42,
                        "column": 29
                      },
                      "end": {
                        "row": 42,
                        "column": 70
                      },
                      "text": "(groupId, description, interruptOnCancel)",
                      "children": [
                        {
                          "type": "(",
                          "start": {
                            "row": 42,
                            "column": 29
                          },
                          "end": {
                            "row": 42,
                            "column": 30
                          },
                          "text": "("
                        },
                        {
                          "type": "identifier",
                          "start": {
                            "row": 42,
                            "column": 30
                          },
                          "end": {
                            "row": 42,
                            "column": 37
                          },
                          "text": "groupId"
                        },
                        {
                          "type": ",",
                          "start": {
                            "row": 42,
                            "column": 37
                          },
                          "end": {
                            "row": 42,
                            "column": 38
                          },
                          "text": ","
                        },
                        {
                          "type": "identifier",
                          "start": {
                            "row": 42,
                            "column": 39
                          },
                          "end": {
                            "row": 42,
                            "column": 50
                          },
                          "text": "description"
                        },
                        {
                          "type": ",",
                          "start": {
                            "row": 42,
                            "column": 50
                          },
                          "end": {
                            "row": 42,
                            "column": 51
                          },
                          "text": ","
                        },
                        {
                          "type": "identifier",
                          "start": {
                            "row": 42,
                            "column": 52
                          },
                          "end": {
                            "row": 42,
                            "column": 69
                          },
                          "text": "interruptOnCancel"
                        },
                        {
                          "type": ")",
                          "start": {
                            "row": 42,
                            "column": 69
                          },
                          "end": {
                            "row": 42,
                            "column": 70
                          },
                          "text": ")"
                        }
                      ]
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    }
  ]
}